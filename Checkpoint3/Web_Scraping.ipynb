{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 3: Web Scraping for Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The goal of this task is to scrape product reviews from a given product URL, extract essential details such as review text, ratings, reviewer names, and dates, and save the results into a structured CSV file for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Used\n",
    "- **requests**: To fetch HTML content of the product page.\n",
    "- **BeautifulSoup**: For parsing and extracting data from HTML.\n",
    "- **pandas**: To organize and save the extracted data.\n",
    "\n",
    "Install the required libraries using:\n",
    "```bash\n",
    "pip install requests beautifulsoup4 pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_25880\\1376981595.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "2025-02-10 14:57:08,661 - INFO - Fetching https://www.amazon.in/ZEBRONICS-Zeb-Thunder-Connectivity-Sea-Green/dp/B09B5CPV71/ref=sr_1_3?sr=8-3...\n",
      "2025-02-10 14:57:12,270 - INFO - Scraped reviews saved to Reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import uniform\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Function to scrape reviews from a product URL\n",
    "def scrape_reviews(product_url, review_selector, text_selector, rating_selector, author_selector, date_selector):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Fetching {product_url}...\")\n",
    "        response = requests.get(product_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching the page: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    reviews = soup.select(review_selector)\n",
    "\n",
    "    review_texts = []\n",
    "    ratings = []\n",
    "    reviewer_names = []\n",
    "    review_dates = []\n",
    "\n",
    "    for review in reviews:\n",
    "        # Extract review text\n",
    "        text = review.select_one(text_selector).get_text(strip=True) if review.select_one(text_selector) else None\n",
    "        review_texts.append(text)\n",
    "\n",
    "        # Extract and clean rating\n",
    "        rating = review.select_one(rating_selector).get_text(strip=True) if review.select_one(rating_selector) else None\n",
    "        ratings.append(clean_rating(rating))\n",
    "\n",
    "        # Extract reviewer name\n",
    "        name = review.select_one(author_selector).get_text(strip=True) if review.select_one(author_selector) else None\n",
    "        reviewer_names.append(name)\n",
    "\n",
    "        # Extract review date\n",
    "        date = review.select_one(date_selector).get_text(strip=True) if review.select_one(date_selector) else None\n",
    "        review_dates.append(date)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Review Text\": review_texts,\n",
    "        \"Rating\": ratings,\n",
    "        \"Reviewer Name\": reviewer_names,\n",
    "        \"Review Date\": review_dates\n",
    "    })\n",
    "\n",
    "# Function to clean rating data\n",
    "def clean_rating(rating):\n",
    "    if rating:\n",
    "        try:\n",
    "            return float(rating.split()[0])\n",
    "        except (ValueError, AttributeError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Main function to scrape and save reviews\n",
    "def main():\n",
    "    product_url = input(\"Enter the product URL to scrape reviews: \").strip()\n",
    "    output_file = \"Reviews.csv\"\n",
    "\n",
    "    # Define selectors (customize for the target website)\n",
    "    selectors = {\n",
    "        \"review_selector\": \".review\",\n",
    "        \"text_selector\": \".review-text\",\n",
    "        \"rating_selector\": \".review-rating\",\n",
    "        \"author_selector\": \".review-author\",\n",
    "        \"date_selector\": \".review-date\"\n",
    "    }\n",
    "\n",
    "    # Scrape reviews\n",
    "    reviews_data = scrape_reviews(product_url, **selectors)\n",
    "\n",
    "    if reviews_data.empty:\n",
    "        logging.error(\"No reviews scraped. Check the URL or selectors.\")\n",
    "        return\n",
    "\n",
    "    # Save to CSV\n",
    "    if os.path.exists(output_file):\n",
    "        logging.warning(f\"{output_file} already exists. Appending data.\")\n",
    "        existing_data = pd.read_csv(output_file)\n",
    "        reviews_data = pd.concat([existing_data, reviews_data], ignore_index=True)\n",
    "\n",
    "    reviews_data.to_csv(output_file, index=False)\n",
    "    logging.info(f\"Scraped reviews saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Run the Script\n",
    "1. **Install Required Libraries**:\n",
    "   ```bash\n",
    "   pip install requests beautifulsoup4 pandas\n",
    "   ```\n",
    "2. **Run the Script**:\n",
    "   - Execute the code and provide a valid product URL when prompted.\n",
    "3. **Output**:\n",
    "   - The extracted reviews will be saved into a file named `Reviews.csv`.\n",
    "   - The file contains fields: Review Text, Rating, Reviewer Name, and Review Date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- The script works for static websites. For dynamic content, use tools like `selenium`.\n",
    "- Ensure the URL provided is from the target website and contains reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
